{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Structured Streaming - Demo\n",
    "## Pizza Oven\n",
    "\n",
    "\n",
    "### Authors\n",
    "\n",
    "```\n",
    "Marco Balduini - marco.balduini@quantiaconsulting.com\n",
    "Emanuele Della Valle - emanuele.dellavalle@polimi.it\n",
    "```\n",
    "```\n",
    "Translation to SSS: Massimo Pavan - massimo1.pavan@mail.polimi.it\n",
    "```\n",
    "\n",
    "### Use Case Description - Linear Pizza Oven\n",
    "We have a linear oven to continuously cook pizza.\n",
    "\n",
    "The cooking operation has two main steps:\n",
    "\n",
    "* the cooking of the pizza base, and\n",
    "* the mozzarella melting area.\n",
    "\n",
    "There are two sensors:\n",
    "\n",
    "* S1 measures the temperature and the relative humidity of the pizza base cooking area.\n",
    "* S2 measures the temperature and the relative humidity of the mozzarella melting area. \n",
    "\n",
    "Both sensors send a temperature measurement every minute, but are not synchronised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.streaming import StreamingContext\n",
    "import io\n",
    "from pyspark.sql.functions import *\n",
    "import time\n",
    "import json\n",
    "import struct\n",
    "import requests \n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.1,org.apache.spark:spark-streaming-kafka-0-10_2.11:2.4.5,org.apache.kafka:kafka-clients:2.6.0 pyspark-shell'\n",
    "                                    \n",
    "spark = (SparkSession.builder \n",
    "    .master(\"local[*]\")\n",
    "    .appName(\"test\")\n",
    "    .getOrCreate()\n",
    "        )\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set up the environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature_humidity_topic = 'Temperature_Humidity_Sensor_Event'\n",
    "servers = \"kafka:9092\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding spark-kafka integration\n",
    "Let's treat first kafka as a bulk source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature_humidity_df = (spark\n",
    "  .read\n",
    "  .format(\"kafka\")\n",
    "  .option(\"kafka.bootstrap.servers\", servers)\n",
    "  .option(\"subscribe\", temperature_humidity_topic)\n",
    "  .option(\"startingOffsets\", \"earliest\")\n",
    "  .option(\"endingOffsets\", \"latest\")\n",
    "  .load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature_humidity_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature_humidity_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stringified_temperature_humidity_df = temperature_humidity_df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n",
    "stringified_temperature_humidity_df.show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "temperature_humidity_schema = StructType([\n",
    "    StructField(\"sensor\", StringType(), True),\n",
    "    StructField(\"temperature\", IntegerType(), True),\n",
    "    StructField(\"humidity\", IntegerType(), True),\n",
    "    StructField(\"ts\", TimestampType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_temperature_humidity_df = stringified_temperature_humidity_df.select(col(\"key\").cast(\"string\"),from_json(col(\"value\"), temperature_humidity_schema).alias(\"value\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_temperature_humidity_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_temperature_humidity_df.select(\"value.*\").show(35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEMO\n",
    "Please refer to [insert_link_here_if_available]() for the EPL version of the following queries.\n",
    "\n",
    "link to docs: https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_temperature_humidity_df = (spark\n",
    "  .readStream\n",
    "  .format(\"kafka\")\n",
    "  .option(\"kafka.bootstrap.servers\", servers)\n",
    "  .option(\"startingOffsets\", \"earliest\")\n",
    "  .option(\"subscribe\", temperature_humidity_topic)\n",
    "  .load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_streaming_temperature_humidity_df=(streaming_temperature_humidity_df\n",
    "                      .select(from_json(col(\"value\").cast(\"string\"), temperature_humidity_schema).alias(\"value\"))\n",
    "                      .select(\"value.*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_streaming_temperature_humidity_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature_humidity_query = (decoded_streaming_temperature_humidity_df\n",
    "    .writeStream\n",
    "    .format(\"memory\")\n",
    "    .queryName(\"temperature_humiditySensorEvent\")\n",
    "    .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM temperature_humiditySensorEvent ORDER BY TS ASC\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1 - Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM temperature_humiditySensorEvent WHERE temperature < 100 AND sensor = 'S2' \").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2 - Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract all the measurements in a given range\n",
    "### Absolute range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM temperature_humiditySensorEvent WHERE ts >= '2020-07-21 12:00:00' AND ts <= '2020-07-21 12:05:00'\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relative range (start: -36h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = 1595333140\n",
    "thirtysixhoursago = datetime.fromtimestamp(now - 60*60*36).strftime(\"%Y-%m-%d %H:%M:%S\") #60*60*36 = seconds*minutes*hours\n",
    "query = \"SELECT * FROM temperature_humiditySensorEvent WHERE ts >= '{}'\".format(thirtysixhoursago)\n",
    "spark.sql(query).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3 - Filter by tag\n",
    "\n",
    "Extract the temperature data from the cooking base area (sensor S1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM temperature_humiditySensorEvent WHERE sensor = 'S1'\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4 - Filter By Value \n",
    "\n",
    "Extract the measurements from the cooking base area (sensor S1) with a temperature under 300°  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM temperature_humiditySensorEvent WHERE sensor = 'S1' AND temperature < 300 \").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5 - Grouping + Aggregator (mean)\n",
    "\n",
    "#### Extract the average temperature and the average humidity along the different stages of the linear pizza oven"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Watermarks are necessary while quering the data, in order to understand how much the data can arrive late \n",
    "All_time_averages_query = (decoded_streaming_temperature_humidity_df\n",
    "                         .withWatermark(\"ts\", \"1 minutes\")\n",
    "                         .groupBy(col(\"sensor\"))\n",
    "                         .avg(\"humidity\", \"temperature\")\n",
    "                     .writeStream\n",
    "                     .outputMode(\"complete\")\n",
    "                     .format(\"memory\")\n",
    "                     .queryName(\"All_time_averages_query\")\n",
    "                     .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The execution of this query could require some time: if dataframe seems empty, just try to re-run the cell after a while\n",
    "spark.sql(\"SELECT * FROM All_time_averages_query\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_time_averages_query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract the last humidity and temperature measurements from the cooking base area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AND is necessary because there can be a record with that ts also from the s2 sensor\n",
    "spark.sql(\"\"\"SELECT * FROM temperature_humiditySensorEvent WHERE ts = (SELECT MAX(ts) FROM temperature_humiditySensorEvent \n",
    "            WHERE sensor = 'S1') AND sensor = 'S1'\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q6 - Aggregate Window\n",
    "\n",
    "#### Extract the moving average temperature observed in the cooking base area over a window of 2 minutes (DEMO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#note: this corresponds to a logical tumbling window\n",
    "LTW_temperature_query = (decoded_streaming_temperature_humidity_df\n",
    "                         .withWatermark(\"ts\", \"1 minutes\")\n",
    "                         .groupBy(window(\"ts\", \"2 minutes\"),\"sensor\")\n",
    "                         .avg(\"temperature\")\n",
    "                     .writeStream\n",
    "                     .format(\"memory\")\n",
    "                     .queryName(\"LTW_temperature_query_results\")\n",
    "                     .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CARE: IT MAY TAKE A WHILE (MINUTES) TO PROCESS THE MOST RECENT WINDOW\n",
    "spark.sql(\"SELECT * FROM LTW_temperature_query_results WHERE sensor = 'S1' \").show(25,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LTW_temperature_query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract the moving average temperature observed by S2 over a window of 3 minutes (hands-on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LTW_temperature_query2 = (decoded_streaming_temperature_humidity_df\n",
    "                         .withWatermark(\"TS\", \"1 minutes\")\n",
    "                         .groupBy(window(\"TS\", \"3 minutes\"),\"sensor\")\n",
    "                         .avg(\"temperature\")\n",
    "                     .writeStream\n",
    "                     .format(\"memory\")\n",
    "                     .queryName(\"LTW_temperature_query2_results\")\n",
    "                     .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM LTW_temperature_query2_results WHERE sensor = 'S2'\").show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LTW_temperature_query2.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q7 - Map and custom function\n",
    "\n",
    "#### Correct the temperature observations of the cooking base area by by subtracting a delta of 5°C to each value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if you want to keep all the records, also the one from the other sensor, a solution could be:\n",
    "\n",
    "new_column = when(\n",
    "        (col(\"sensor\") == \"S1\"), col(\"temperature\") - 5\n",
    "    ).otherwise(col(\"temperature\"))\n",
    "\n",
    "map_temperature_query = (decoded_streaming_temperature_humidity_df\n",
    "                         .withColumn(\"temperature\", new_column)\n",
    "                     .writeStream\n",
    "                     .format(\"memory\")\n",
    "                     .queryName(\"map_temperature_query_results\")\n",
    "                     .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM map_temperature_query_results\").show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_temperature_query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alternatively, if you'd like to keep only the values from sensor S1 a solution could be:\n",
    "def sub5(x):\n",
    "    x = x-5\n",
    "    return x\n",
    "\n",
    "df = decoded_streaming_temperature_humidity_df.select(\"*\").where(\"sensor = 'S1'\")\n",
    "fun = udf(sub5)\n",
    "\n",
    "map_temperature_query_alt = (df\n",
    "                         .withColumn(\"temperature\", fun(df[\"temperature\"]))\n",
    "                     .writeStream\n",
    "                     .format(\"memory\")\n",
    "                     .queryName(\"map_temperature_query_alt_results\")\n",
    "                     .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM map_temperature_query_alt_results\").show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_temperature_query_alt.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q8 - Stream-to-Stream Join\n",
    "\n",
    "#### Extract the difference between the temperature of the base cooking area and the mozzarella melting area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join assuming synchronous time-series\n",
    "\n",
    "Apply watermarks on event-time columns and other filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_S1_events = (decoded_streaming_temperature_humidity_df\n",
    "                .withWatermark(\"ts\", \"1 minute\")\n",
    "                .filter(col(\"sensor\") == \"S1\")\n",
    "               )\n",
    "\n",
    "only_S2_events = (decoded_streaming_temperature_humidity_df\n",
    "                .withWatermark(\"ts\", \"1 minute\")\n",
    "                .filter(col(\"sensor\") == \"S2\")\n",
    "               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join with event-time constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_df = (only_S1_events.join(\n",
    "  only_S2_events,\n",
    "    (only_S1_events.ts == only_S2_events.ts)) \n",
    "           .select(only_S1_events.temperature,\n",
    "                   only_S2_events.temperature,\n",
    "                   only_S1_events.humidity,\n",
    "                   only_S2_events.humidity,\n",
    "                   only_S1_events.ts\n",
    "                  ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_to_s_join_query = (join_df\n",
    "                     .writeStream\n",
    "                     .format(\"memory\")\n",
    "                     .queryName(\"s_to_s_join_query_results\")\n",
    "                     .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM s_to_s_join_query_results ORDER BY ts DESC\").show(5,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT:** If we simply try to join on the ts the df will always be empty, since the records are not sincronized!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_to_s_join_query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join assuming a fixed delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_S1_events = (decoded_streaming_temperature_humidity_df\n",
    "                .filter(col(\"sensor\") == \"S1\")\n",
    "                .select(col(\"ts\").alias(\"S1_ts\"), \n",
    "                        col(\"temperature\").alias(\"S1_temperature\"), col(\"humidity\").alias(\"S1_humidity\"))\n",
    "                .withWatermark(\"S1_ts\", \"1 minutes\")\n",
    "               )\n",
    "\n",
    "only_S2_events = (decoded_streaming_temperature_humidity_df\n",
    "                .filter(col(\"sensor\") == \"S2\")\n",
    "                .select(col(\"ts\").alias(\"S2_ts\"), \n",
    "                        col(\"temperature\").alias(\"S2_temperature\"), col(\"humidity\").alias(\"S2_humidity\"))\n",
    "                .withWatermark(\"S2_ts\", \"1 minutes\")\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_S1_query = (only_S1_events\n",
    "                     .writeStream\n",
    "                     .format(\"memory\")\n",
    "                     .queryName(\"results1\")\n",
    "                     .start())\n",
    "\n",
    "only_S2_query = (only_S2_events\n",
    "                     .writeStream\n",
    "                     .format(\"memory\")\n",
    "                     .queryName(\"results2\")\n",
    "                     .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#join \n",
    "df = spark.sql(\"SELECT * FROM results1 join results2 ON S1_ts <= (S2_ts + INTERVAL 4 seconds) AND S1_ts >= S2_ts\")\n",
    "df.show(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "#Alternative way for using user defined functions\n",
    "\n",
    "@udf(\"int\")\n",
    "def diff(x, y):\n",
    "    return x - y\n",
    "\n",
    "#Calculating difference\n",
    "df.withColumn(\"difference\", diff(df[\"S1_temperature\"], df[\"S2_temperature\"])).show(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_S2_query.stop()\n",
    "only_S1_query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join exploiting time-windows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#note: to demonstrate the use of a different time-window, for this query a LOGICAL HOPPING WINDOW HAVE BEEN USED\n",
    "only_S1_wind_events = (decoded_streaming_temperature_humidity_df\n",
    "                .filter(col(\"sensor\") == \"S1\")\n",
    "                .select(col(\"ts\").alias(\"S1_ts\"), \n",
    "                        col(\"temperature\").alias(\"S1_temperature\"), col(\"humidity\").alias(\"S1_humidity\"))\n",
    "                .withWatermark(\"S1_ts\", \"1 minutes\")\n",
    "                       .groupBy(window(\"S1_ts\", \"1 minutes\", \"30 seconds\"))\n",
    "                       .avg(\"S1_humidity\")\n",
    "               )\n",
    "\n",
    "only_S2_wind_events = (decoded_streaming_temperature_humidity_df\n",
    "                .filter(col(\"sensor\") == \"S2\")\n",
    "                .select(col(\"ts\").alias(\"S2_ts\"), \n",
    "                        col(\"temperature\").alias(\"S2_temperature\"), col(\"humidity\").alias(\"S2_humidity\"))\n",
    "                .withWatermark(\"S2_ts\", \"1 minutes\")\n",
    "                       .groupBy(window(\"S2_ts\", \"1 minutes\", \"30 seconds\"))\n",
    "                       .avg(\"S2_humidity\")\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_S1_wind_query = (only_S1_wind_events\n",
    "                     .writeStream\n",
    "                     .format(\"memory\")\n",
    "                     .queryName(\"results1\")\n",
    "                     .start())\n",
    "\n",
    "only_S2_wind_query = (only_S2_wind_events\n",
    "                     .writeStream\n",
    "                     .format(\"memory\")\n",
    "                     .queryName(\"results2\")\n",
    "                     .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#join \n",
    "df = spark.sql(\"SELECT * FROM results1 join results2 ON results1.window = results2.window\")\n",
    "df.show(45, truncate = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating difference\n",
    "df = df.withColumn(\"difference\", fun(df[\"avg(S2_humidity)\"], df[\"avg(S1_humidity)\"]))\n",
    "df.show(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract the difference between the humidity levels of the base cooking area and the mozzarella melting area. Find if the differences are between 20 and 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(df[\"difference\"] > 20).filter(df[\"difference\"] < 30).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_S1_wind_query.stop()\n",
    "only_S2_wind_query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q9 - static-streaming join df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "consider the following data are store in a DB\n",
    "\n",
    "```\n",
    "CREATE DATABASE pizza-erp;\n",
    "\n",
    "CREATE TABLE public.oven\n",
    "(\n",
    "    pid bigint NOT NULL,\n",
    "    kind character varying COLLATE pg_catalog.\"default\" NOT NULL,\n",
    "    enteringtime bigint NOT NULL,\n",
    "    exitingtime bigint,\n",
    "    sensor character varying COLLATE pg_catalog.\"default\" NOT NULL,\n",
    "    CONSTRAINT hoven_pkey PRIMARY KEY (pid,enteringtime,sensor)\n",
    ");\n",
    "\n",
    "INSERT INTO oven (pid,kind,enteringtime,exitingtime,sensor) VALUES(2,'napoli',1602504000000000000,1602504150000000000,'S1');\n",
    "INSERT INTO oven (pid,kind,enteringtime,exitingtime,sensor) VALUES(1,'margherita',1602504010000000000,1602504080000000000,'S2');\n",
    "INSERT INTO oven (pid,kind,enteringtime,exitingtime,sensor) VALUES(3,'pepperoni',1602504170000000000,1602504250000000000,'S1');\n",
    "INSERT INTO oven (pid,kind,enteringtime,exitingtime,sensor) VALUES(2,'napoli',1602504130000000000,1602504284000000000,'S2');\n",
    "```\n",
    "\n",
    "enrich the time-serires with the data in the DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n",
    "#create the static-df\n",
    "pizza_df = sc.parallelize([\n",
    "    [2,'napoli', 1595332800,1595332960,'S1'],\n",
    "    [1,'margherita',1595332810,1595332935,'S2'],\n",
    "    [3,'pepperoni',1595332980,1595333060,'S1'],\n",
    "    [2,'napoli',1595332960,1595333095,'S2']]\n",
    ").toDF([\"pid\",\"kind\",\"enteringtime\",\"exitingtime\",\"sensor\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cast time from unix to ts format\n",
    "\n",
    "pizza_df = pizza_df.withColumn(\"enteringtime\", to_timestamp(pizza_df[\"enteringtime\"]))\n",
    "pizza_df = pizza_df.withColumn(\"exitingtime\", to_timestamp(pizza_df[\"exitingtime\"]))\n",
    "pizza_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_df = decoded_streaming_temperature_humidity_df.join(pizza_df, (pizza_df.sensor == decoded_streaming_temperature_humidity_df.sensor) & \n",
    "                                                         (pizza_df.enteringtime <= decoded_streaming_temperature_humidity_df.ts) & \n",
    "                                                         (pizza_df.exitingtime >= decoded_streaming_temperature_humidity_df.ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_query = (join_df\n",
    "    .writeStream\n",
    "    .format(\"memory\")\n",
    "    .queryName(\"join_Event\")\n",
    "    .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.sql(\"SELECT * FROM join_Event\")\n",
    "df.show(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature_humidity_query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}