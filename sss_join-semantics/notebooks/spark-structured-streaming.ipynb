{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crossing the Streams â€“ Joins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use an example to demonstrate the differences in the joins. It is based on the online advertising domain. There is a Kafka topic that contains view events of particular ads and another one that contains click events based on those ads. Views and clicks share an ID that serves as the key in both topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apache Spark joins**:\n",
    "\n",
    "- Stream-static joins: INNER and LEFT-OUTER stream-static joins are not statuful; OUTER and RIGHT-OUTER are not supported\n",
    "\n",
    "- Stream-stream joins:\n",
    "    - Inner: Supported, optionally specify watermark on both sides + time constraints for state cleanup\n",
    "    - Left Outer: Conditionally supported, must specify watermark on right + time constraints for correct results, optionally specify watermark on left for all state cleanup\n",
    "    - Right Outer: Conditionally supported, must specify watermark on left + time constraints for correct results, optionally specify watermark on right for all state cleanup\n",
    "    - Full Outer: Not supported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://8dc37c7e5c11:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>test</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f6314444490>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.streaming import StreamingContext\n",
    "import io\n",
    "from pyspark.sql.functions import *\n",
    "import time\n",
    "import json\n",
    "import struct\n",
    "import requests \n",
    "from IPython.display import clear_output\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.1,org.apache.spark:spark-streaming-kafka-0-10_2.11:2.4.5,org.apache.kafka:kafka-clients:2.6.0 pyspark-shell'\n",
    "                                    \n",
    "spark = (SparkSession.builder \n",
    "    .master(\"local[*]\")\n",
    "    .appName(\"test\")\n",
    "    .getOrCreate()\n",
    "        )\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set up the environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "advertisement_topic = 'ParticularAdEvent'\n",
    "click_topic = 'ClickEvent'\n",
    "servers = \"kafka:9092\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's create the streaming Data Frames using the data in the kafka topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_streaming_ad_df = (spark\n",
    "  .readStream\n",
    "  .format(\"kafka\")\n",
    "  .option(\"kafka.bootstrap.servers\", servers)\n",
    "  .option(\"startingOffsets\", \"earliest\")\n",
    "  .option(\"subscribe\", advertisement_topic)\n",
    "  .load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_streaming_ad_df.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_streaming_ad_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "ad_schema = StructType([\n",
    "    StructField(\"ad\", StringType(), True),\n",
    "    StructField(\"ts\", TimestampType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_sdf=(raw_streaming_ad_df\n",
    "                      .select(from_json(col(\"value\").cast(\"string\"), ad_schema).alias(\"value\"))\n",
    "                      .select(\"value.*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ad: string (nullable = true)\n",
      " |-- ts: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ad_sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "click_schema = StructType([\n",
    "    StructField(\"choose_ad\", StringType(), True),\n",
    "    StructField(\"ts\", TimestampType(), True)])\n",
    "\n",
    "raw_streaming_click_df = (spark\n",
    "  .readStream\n",
    "  .format(\"kafka\")\n",
    "  .option(\"kafka.bootstrap.servers\", servers)\n",
    "  .option(\"startingOffsets\", \"earliest\")\n",
    "  .option(\"subscribe\", click_topic)\n",
    "  .load())\n",
    "\n",
    "click_sdf = (raw_streaming_click_df\n",
    "                      .select(from_json(col(\"value\").cast(\"string\"), click_schema).alias(\"value\"))\n",
    "                      .select(\"value.*\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before we start to work with queries, let's inspect the content of our advertisement stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_query = (ad_sdf\n",
    "    .writeStream\n",
    "    .format(\"memory\") # this is for debug purpose only! DO NOT USE IN PRODUCTION\n",
    "    .queryName(\"sinkTable_basic\")\n",
    "    .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+\n",
      "| ad|                 ts|\n",
      "+---+-------------------+\n",
      "|  G|2022-04-26 10:41:18|\n",
      "|  F|2022-04-26 10:41:12|\n",
      "|  D|2022-04-26 10:41:06|\n",
      "|  C|2022-04-26 10:41:00|\n",
      "|  B|2022-04-26 10:40:54|\n",
      "+---+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM sinkTable_basic ORDER BY TS DESC\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1 - Inner Stream-Stream Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_minute_ad_events = (ad_sdf\n",
    "                .withWatermark(\"ts\", \"1 minute\")\n",
    "                .withColumnRenamed(\"ad\",\"advertisement\")\n",
    "                .withColumnRenamed(\"ts\",\"tsAdvertisement\")\n",
    "               )\n",
    "\n",
    "last_minute_click_events = (click_sdf\n",
    "                .withWatermark(\"ts\", \"1 minute\")\n",
    "                .withColumnRenamed(\"ts\",\"tsClick\")\n",
    "               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the interval equal to 10 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_sdf = (last_minute_ad_events.join(\n",
    "  last_minute_click_events, expr(\"\"\"\n",
    "    (advertisement == choose_ad) AND\n",
    "    (tsClick > tsAdvertisement ) AND\n",
    "    (tsClick < tsAdvertisement + interval 10 seconds )\n",
    "    \"\"\"\n",
    "    )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = (join_sdf\n",
    "            .writeStream\n",
    "            .format(\"memory\")\n",
    "            .queryName(\"sinkTable\")\n",
    "            .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+---------+-------------------+\n",
      "|advertisement|tsAdvertisement    |choose_ad|tsClick            |\n",
      "+-------------+-------------------+---------+-------------------+\n",
      "|G            |2022-04-26 10:40:42|G        |2022-04-26 10:40:48|\n",
      "|D            |2022-04-26 10:39:54|D        |2022-04-26 10:40:02|\n",
      "|B            |2022-04-26 10:39:06|B        |2022-04-26 10:39:07|\n",
      "|G            |2022-04-26 10:38:17|G        |2022-04-26 10:38:22|\n",
      "|C            |2022-04-26 10:36:47|C        |2022-04-26 10:36:51|\n",
      "|A            |2022-04-26 10:35:59|A        |2022-04-26 10:36:08|\n",
      "+-------------+-------------------+---------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM sinkTable ORDER BY tsClick DESC\").show(20,False) # note, I change ts in tsClick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2 - Left Stream-Stream Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_minute_ad_events_left = (ad_sdf\n",
    "                .withWatermark(\"ts\", \"3 seconds\")\n",
    "                .withColumnRenamed(\"ad\",\"advertisement\")\n",
    "                .withColumnRenamed(\"ts\",\"tsAdvertisement\")\n",
    "               )\n",
    "\n",
    "last_minute_click_events_left = (click_sdf\n",
    "                .withWatermark(\"ts\", \"3 seconds\")\n",
    "                .withColumnRenamed(\"ts\",\"tsClick\")\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_sdf_second = (last_minute_ad_events_left.join(\n",
    "  last_minute_click_events_left, expr(\"\"\"\n",
    "    (advertisement == choose_ad) AND\n",
    "    (tsClick > tsAdvertisement ) AND\n",
    "    (tsClick < tsAdvertisement + interval 3 seconds )\n",
    "    \"\"\"\n",
    "    ), \"leftOuter\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "q2 = (join_sdf_second\n",
    "            .writeStream\n",
    "            .format(\"memory\")\n",
    "            .queryName(\"sinkTable\")\n",
    "            .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+---------+-------------------+\n",
      "|advertisement|    tsAdvertisement|choose_ad|            tsClick|\n",
      "+-------------+-------------------+---------+-------------------+\n",
      "|            B|2022-04-26 10:39:06|        B|2022-04-26 10:39:07|\n",
      "|            F|2022-04-26 10:35:47|     null|               null|\n",
      "|            F|2022-04-26 10:41:48|     null|               null|\n",
      "|            F|2022-04-26 10:36:23|     null|               null|\n",
      "|            F|2022-04-26 10:39:24|     null|               null|\n",
      "|            F|2022-04-26 10:41:12|     null|               null|\n",
      "|            F|2022-04-26 10:40:36|     null|               null|\n",
      "|            F|2022-04-26 10:40:00|     null|               null|\n",
      "|            F|2022-04-26 10:38:48|     null|               null|\n",
      "|            F|2022-04-26 10:38:11|     null|               null|\n",
      "+-------------+-------------------+---------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM sinkTable ORDER BY tsClick DESC\").show(10) # note, I change ts in tsClick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "q2.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3 - Outer Stream-Stream Join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full outer join is not supported while doing stream/stream joins in Apache Spark Structured Streaming. To show that it is true, we should execute the following cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_sdf = (last_minute_ad_events.join(\n",
    "  last_minute_click_events, expr(\"\"\"\n",
    "    (advertisement == choose_ad) AND\n",
    "    (tsClick > tsAdvertisement ) AND\n",
    "    (tsClick < tsAdvertisement + interval 10 seconds )\n",
    "    \"\"\"\n",
    "    ), \"fullOuter\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Full outer joins with streaming DataFrames/Datasets are not supported;;\nJoin FullOuter, (((advertisement#115 = choose_ad#50) AND (tsClick#121-T60000ms > tsAdvertisement#118-T60000ms)) AND (tsClick#121-T60000ms < cast(tsAdvertisement#118-T60000ms + 10 seconds as timestamp)))\n:- Project [advertisement#115, ts#24-T60000ms AS tsAdvertisement#118-T60000ms]\n:  +- Project [ad#23 AS advertisement#115, ts#24-T60000ms]\n:     +- EventTimeWatermark ts#24: timestamp, 1 minutes\n:        +- Project [value#21.ad AS ad#23, value#21.ts AS ts#24]\n:           +- Project [from_json(StructField(ad,StringType,true), StructField(ts,TimestampType,true), cast(value#8 as string), Some(Etc/UTC)) AS value#21]\n:              +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@68fb8728, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@6c87f986, org.apache.spark.sql.util.CaseInsensitiveStringMap@fdaffb46, [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@653a5e68,kafka,List(),None,List(),None,Map(startingOffsets -> earliest, subscribe -> ParticularAdEvent, kafka.bootstrap.servers -> kafka:9092),None), kafka, [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6]\n+- Project [choose_ad#50, ts#51-T60000ms AS tsClick#121-T60000ms]\n   +- EventTimeWatermark ts#51: timestamp, 1 minutes\n      +- Project [value#48.choose_ad AS choose_ad#50, value#48.ts AS ts#51]\n         +- Project [from_json(StructField(choose_ad,StringType,true), StructField(ts,TimestampType,true), cast(value#35 as string), Some(Etc/UTC)) AS value#48]\n            +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@7af897cc, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@743f76e3, org.apache.spark.sql.util.CaseInsensitiveStringMap@f5ec8bd6, [key#34, value#35, topic#36, partition#37, offset#38L, timestamp#39, timestampType#40], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@653a5e68,kafka,List(),None,List(),None,Map(startingOffsets -> earliest, subscribe -> ClickEvent, kafka.bootstrap.servers -> kafka:9092),None), kafka, [key#27, value#28, topic#29, partition#30, offset#31L, timestamp#32, timestampType#33]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-9c860e6a2784>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m q3 = (join_sdf\n\u001b[0m\u001b[1;32m      2\u001b[0m             \u001b[0;34m.\u001b[0m\u001b[0mwriteStream\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m             \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m             \u001b[0;34m.\u001b[0m\u001b[0mqueryName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sinkTable\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             .start())\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/streaming.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self, path, format, outputMode, partitionBy, queryName, **options)\u001b[0m\n\u001b[1;32m   1209\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueryName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqueryName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1210\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1212\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1213\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Full outer joins with streaming DataFrames/Datasets are not supported;;\nJoin FullOuter, (((advertisement#115 = choose_ad#50) AND (tsClick#121-T60000ms > tsAdvertisement#118-T60000ms)) AND (tsClick#121-T60000ms < cast(tsAdvertisement#118-T60000ms + 10 seconds as timestamp)))\n:- Project [advertisement#115, ts#24-T60000ms AS tsAdvertisement#118-T60000ms]\n:  +- Project [ad#23 AS advertisement#115, ts#24-T60000ms]\n:     +- EventTimeWatermark ts#24: timestamp, 1 minutes\n:        +- Project [value#21.ad AS ad#23, value#21.ts AS ts#24]\n:           +- Project [from_json(StructField(ad,StringType,true), StructField(ts,TimestampType,true), cast(value#8 as string), Some(Etc/UTC)) AS value#21]\n:              +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@68fb8728, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@6c87f986, org.apache.spark.sql.util.CaseInsensitiveStringMap@fdaffb46, [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@653a5e68,kafka,List(),None,List(),None,Map(startingOffsets -> earliest, subscribe -> ParticularAdEvent, kafka.bootstrap.servers -> kafka:9092),None), kafka, [key#0, value#1, topic#2, partition#3, offset#4L, timestamp#5, timestampType#6]\n+- Project [choose_ad#50, ts#51-T60000ms AS tsClick#121-T60000ms]\n   +- EventTimeWatermark ts#51: timestamp, 1 minutes\n      +- Project [value#48.choose_ad AS choose_ad#50, value#48.ts AS ts#51]\n         +- Project [from_json(StructField(choose_ad,StringType,true), StructField(ts,TimestampType,true), cast(value#35 as string), Some(Etc/UTC)) AS value#48]\n            +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@7af897cc, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@743f76e3, org.apache.spark.sql.util.CaseInsensitiveStringMap@f5ec8bd6, [key#34, value#35, topic#36, partition#37, offset#38L, timestamp#39, timestampType#40], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@653a5e68,kafka,List(),None,List(),None,Map(startingOffsets -> earliest, subscribe -> ClickEvent, kafka.bootstrap.servers -> kafka:9092),None), kafka, [key#27, value#28, topic#29, partition#30, offset#31L, timestamp#32, timestampType#33]\n"
     ]
    }
   ],
   "source": [
    "q3 = (join_sdf\n",
    "            .writeStream\n",
    "            .format(\"memory\")\n",
    "            .queryName(\"sinkTable\")\n",
    "            .start())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation to create table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature of joining static tables is supported in Apache SSS but it is **not the same** as **joining Ktables**. \n",
    "\n",
    "Before we start with joining KTable & KTable, we should remember that Apache Spark doesn't have a totally equivalent structure. To create a similar structure for Spark to use, we should respect the following characteristics of KTable:\n",
    "\n",
    "- KTable contains the latest message only for the corresponding key\n",
    "- KTable is the always updating table, we can view it as the changelog.\n",
    "\n",
    "To create a similar structure, we should:\n",
    "\n",
    "1) Make a spark static table \n",
    "\n",
    "2) Updating it by batches\n",
    "\n",
    "3) Drop the key duplicate\n",
    "\n",
    "But with creating a spark static table, we will read only the earliest data. We cannot put in startingOffsets - \"latest\" because this startingOffset is not available for reading batches. \n",
    "\n",
    "So, our implementation will be a little bit tricky, since we **keep** the previous values while joining (because as told earlier we can't just read the latest value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_static_tables():\n",
    "    raw_static_ad_df = (spark\n",
    "        .read\n",
    "        .format(\"kafka\")\n",
    "        .option(\"kafka.bootstrap.servers\", servers)\n",
    "        .option(\"startingOffsets\", \"earliest\")\n",
    "        .option(\"subscribe\", advertisement_topic)\n",
    "        .load())\n",
    "\n",
    "    ad_static_df=(raw_static_ad_df\n",
    "                      .select(from_json(col(\"value\").cast(\"string\"), ad_schema).alias(\"value\"))\n",
    "                      .select(\"value.*\"))\n",
    "\n",
    "    raw_static_click_df = (spark\n",
    "          .read\n",
    "          .format(\"kafka\")\n",
    "          .option(\"kafka.bootstrap.servers\", servers)\n",
    "          .option(\"startingOffsets\", \"earliest\")\n",
    "          .option(\"subscribe\", click_topic)\n",
    "          .load())\n",
    "\n",
    "    click_static_df = (raw_static_click_df\n",
    "                      .select(from_json(col(\"value\").cast(\"string\"), click_schema).alias(\"value\"))\n",
    "                      .select(\"value.*\"))\n",
    "    \n",
    "    return ad_static_df, click_static_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4 - Table-Table Join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT**:\n",
    "You have to wait here a little bit longer than in streaming case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-d18c91cd3f80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m         ), \"inner\"))\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mjoin_sdf_fourth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoin_sdf_fourth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtsClick\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdesc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"advertisement\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mclear_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    440\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 442\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1301\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1303\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1033\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1034\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1200\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1201\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    ad_static_df, click_static_df = create_static_tables()\n",
    "    \n",
    "    last_minute_ad_events_static = (ad_static_df\n",
    "                .withColumnRenamed(\"ad\",\"advertisement\")\n",
    "                .withColumnRenamed(\"ts\",\"tsAdvertisement\")\n",
    "               )\n",
    "\n",
    "    last_minute_click_events_static = (click_static_df\n",
    "                .withColumnRenamed(\"ts\",\"tsClick\")\n",
    "               )\n",
    "    \n",
    "    join_sdf_fourth = (last_minute_ad_events_static.join(\n",
    "        last_minute_click_events_static, expr(\"\"\"\n",
    "            (advertisement == choose_ad) AND\n",
    "            (tsClick > tsAdvertisement ) AND\n",
    "            (tsClick < tsAdvertisement + interval 10 seconds )\n",
    "            \"\"\"\n",
    "        ), \"inner\"))\n",
    "    \n",
    "    join_sdf_fourth.sort(join_sdf_fourth.tsClick.desc()).drop_duplicates([\"advertisement\"]).show(10, False)\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5 - Left Table-Table Join\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT**:\n",
    "You have to wait here a little bit longer than in streaming case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-ac02ad9b5b6f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m         ), \"leftOuter\"))    \n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mjoin_sdf_five\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoin_sdf_five\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtsClick\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdesc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"advertisement\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mclear_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    440\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 442\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1301\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1303\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1033\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1034\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1200\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1201\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    ad_static_df, click_static_df = create_static_tables()\n",
    "    \n",
    "    last_minute_ad_events_static = (ad_static_df\n",
    "                .withColumnRenamed(\"ad\",\"advertisement\")\n",
    "                .withColumnRenamed(\"ts\",\"tsAdvertisement\")\n",
    "               )\n",
    "\n",
    "    last_minute_click_events_static = (click_static_df\n",
    "                .withColumnRenamed(\"ts\",\"tsClick\")\n",
    "               )\n",
    "    \n",
    "    join_sdf_five = (last_minute_ad_events_static.join(\n",
    "      last_minute_click_events_static, expr(\"\"\"\n",
    "        (advertisement == choose_ad) AND\n",
    "        (tsClick > tsAdvertisement ) AND\n",
    "        (tsClick < tsAdvertisement + interval 5 seconds )\n",
    "        \"\"\"\n",
    "        ), \"leftOuter\"))    \n",
    "    \n",
    "    join_sdf_five.sort(join_sdf_five.tsClick.desc()).drop_duplicates([\"advertisement\"]).show(10, False)\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6 - Outer Table-Table Join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT**:\n",
    "You have to wait here a little bit longer than in streaming case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-201a90e31731>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m         ), \"fullOuter\"))    \n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mjoin_sdf_six\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoin_sdf_six\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtsClick\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdesc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"advertisement\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mclear_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    440\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 442\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1301\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1303\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1033\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1034\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1200\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1201\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    ad_static_df, click_static_df = create_static_tables()\n",
    "    \n",
    "    last_minute_ad_events_static = (ad_static_df\n",
    "                .withColumnRenamed(\"ad\",\"advertisement\")\n",
    "                .withColumnRenamed(\"ts\",\"tsAdvertisement\")\n",
    "               )\n",
    "\n",
    "    last_minute_click_events_static = (click_static_df\n",
    "                .withColumnRenamed(\"ts\",\"tsClick\")\n",
    "               )\n",
    "    \n",
    "    join_sdf_six = (last_minute_ad_events_static.join(\n",
    "      last_minute_click_events_static, expr(\"\"\"\n",
    "        (advertisement == choose_ad) AND\n",
    "        (tsClick > tsAdvertisement ) AND\n",
    "        (tsClick < tsAdvertisement + interval 10 seconds )\n",
    "        \"\"\"\n",
    "        ), \"fullOuter\"))    \n",
    "    \n",
    "    join_sdf_six.sort(join_sdf_six.tsClick.desc()).drop_duplicates([\"advertisement\"]).show(10, False)\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q7 - Inner Stream-Table Join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The assumption here is the following:\n",
    "- The data that is going from advertisement is static, meaning it is passive, the user is active, so it is modeled as the stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_static_ad_df = (spark\n",
    "        .read\n",
    "        .format(\"kafka\")\n",
    "        .option(\"kafka.bootstrap.servers\", servers)\n",
    "        .option(\"startingOffsets\", \"earliest\")\n",
    "        .option(\"subscribe\", advertisement_topic)\n",
    "        .load())\n",
    "\n",
    "ad_static_df=(raw_static_ad_df\n",
    "                      .select(from_json(col(\"value\").cast(\"string\"), ad_schema).alias(\"value\"))\n",
    "                      .select(\"value.*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_minute_ad_events = (ad_static_df\n",
    "                .withColumnRenamed(\"ad\",\"advertisement\")\n",
    "                .withColumnRenamed(\"ts\",\"tsAdvertisement\")\n",
    "               )\n",
    "\n",
    "last_minute_click_events = (click_sdf\n",
    "                .withWatermark(\"ts\", \"1 minute\")\n",
    "                .withColumnRenamed(\"ts\",\"tsClick\")\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_sdf_seven = (last_minute_click_events.join(\n",
    "  last_minute_ad_events, expr(\"\"\"\n",
    "    (advertisement == choose_ad) AND\n",
    "    (tsClick > tsAdvertisement ) AND\n",
    "    (tsClick < tsAdvertisement + interval 10 seconds )\n",
    "    \"\"\"\n",
    "    )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "q7 = (join_sdf_seven\n",
    "            .writeStream\n",
    "            .format(\"memory\")\n",
    "            .queryName(\"sinkTable\")\n",
    "            .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+-------------+-------------------+\n",
      "|choose_ad|tsClick            |advertisement|tsAdvertisement    |\n",
      "+---------+-------------------+-------------+-------------------+\n",
      "|D        |2022-04-26 10:48:22|D            |2022-04-26 10:48:19|\n",
      "|C        |2022-04-26 10:47:45|C            |2022-04-26 10:47:36|\n",
      "|D        |2022-04-26 10:45:23|D            |2022-04-26 10:45:18|\n",
      "|B        |2022-04-26 10:41:38|B            |2022-04-26 10:41:30|\n",
      "|G        |2022-04-26 10:40:48|G            |2022-04-26 10:40:42|\n",
      "|D        |2022-04-26 10:40:02|D            |2022-04-26 10:39:54|\n",
      "|B        |2022-04-26 10:39:07|B            |2022-04-26 10:39:06|\n",
      "|G        |2022-04-26 10:38:22|G            |2022-04-26 10:38:17|\n",
      "|C        |2022-04-26 10:36:51|C            |2022-04-26 10:36:47|\n",
      "|A        |2022-04-26 10:36:08|A            |2022-04-26 10:35:59|\n",
      "+---------+-------------------+-------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM sinkTable ORDER BY tsClick DESC\").show(20,False) # note, I change ts in tsClick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "q7.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q8 - Left Stream-Table Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_minute_ad_events = (ad_static_df\n",
    "                .withColumnRenamed(\"ad\",\"advertisement\")\n",
    "                .withColumnRenamed(\"ts\",\"tsAdvertisement\")\n",
    "               )\n",
    "\n",
    "last_minute_click_events = (click_sdf\n",
    "                .withWatermark(\"ts\", \"1 minute\")\n",
    "                .withColumnRenamed(\"ts\",\"tsClick\")\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_sdf_eight = (last_minute_click_events.join(\n",
    "  last_minute_ad_events, expr(\"\"\"\n",
    "    (advertisement == choose_ad) AND\n",
    "    (tsClick > tsAdvertisement ) AND\n",
    "    (tsClick < tsAdvertisement + interval 10 seconds )\n",
    "    \"\"\"\n",
    "    ), \"leftOuter\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "q8 = (join_sdf_eight\n",
    "            .writeStream\n",
    "            .format(\"memory\")\n",
    "            .queryName(\"sinkTable\")\n",
    "            .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+-------------+-------------------+\n",
      "|choose_ad|tsClick            |advertisement|tsAdvertisement    |\n",
      "+---------+-------------------+-------------+-------------------+\n",
      "|A        |2022-04-26 10:49:32|null         |null               |\n",
      "|G        |2022-04-26 10:48:55|null         |null               |\n",
      "|F        |2022-04-26 10:48:35|null         |null               |\n",
      "|D        |2022-04-26 10:48:22|D            |2022-04-26 10:48:19|\n",
      "|C        |2022-04-26 10:47:45|C            |2022-04-26 10:47:36|\n",
      "|B        |2022-04-26 10:47:16|null         |null               |\n",
      "|A        |2022-04-26 10:46:44|null         |null               |\n",
      "|G        |2022-04-26 10:46:28|null         |null               |\n",
      "|F        |2022-04-26 10:45:57|null         |null               |\n",
      "|D        |2022-04-26 10:45:23|D            |2022-04-26 10:45:18|\n",
      "|C        |2022-04-26 10:44:55|null         |null               |\n",
      "|B        |2022-04-26 10:44:17|null         |null               |\n",
      "|A        |2022-04-26 10:43:44|null         |null               |\n",
      "|G        |2022-04-26 10:43:33|null         |null               |\n",
      "|F        |2022-04-26 10:43:14|null         |null               |\n",
      "|D        |2022-04-26 10:42:41|null         |null               |\n",
      "|C        |2022-04-26 10:42:06|null         |null               |\n",
      "|B        |2022-04-26 10:41:38|B            |2022-04-26 10:41:30|\n",
      "|A        |2022-04-26 10:41:21|null         |null               |\n",
      "|G        |2022-04-26 10:40:48|G            |2022-04-26 10:40:42|\n",
      "+---------+-------------------+-------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM sinkTable ORDER BY tsClick DESC\").show(20,False) # note, I change ts in tsClick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "q8.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q9 - Inner Stream-GlobalKTable Join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Natively **GlobalKTable** is **not supported** in Apache SSS.\n",
    "\n",
    "GlobalKTable should be populated before any processing is done we should have a very big buffer which, of course in real processing is hard to choose because of its size mainly, we need to know in advance its size.\n",
    "\n",
    "To model it we can use a static dataframe, that has already all the data inside of it, **BUT** this solution is infeasible in the real world. Meaning that we will do again **static-stream** join that is supported in Apache Spark Structured Streaming to emulate GlobalKTable\n",
    "\n",
    "Again we assume that the user is active when the advertisement are passive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT**: Notice that in Q9 and Q10 time intervals increased it is done due to simulator of click events that has the delay of user clicking on the advertisement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n",
    "\n",
    "#create the static-df\n",
    "global_table = sc.parallelize([\n",
    "    ['A', time.time()],\n",
    "    ['B', time.time()],\n",
    "    ['C', time.time()],\n",
    "    ['D', time.time()],\n",
    "    ['F', time.time()],\n",
    "    ['G', time.time()],\n",
    "]\n",
    ").toDF([\"advertisement\",\"tsAdvertisement\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------------+\n",
      "|advertisement|tsAdvertisement           |\n",
      "+-------------+--------------------------+\n",
      "|A            |2022-04-26 10:51:27.604575|\n",
      "|B            |2022-04-26 10:51:27.604576|\n",
      "|C            |2022-04-26 10:51:27.604576|\n",
      "|D            |2022-04-26 10:51:27.604576|\n",
      "+-------------+--------------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "global_table = global_table.withColumn(\"tsAdvertisement\", to_timestamp(global_table[\"tsAdvertisement\"]))\n",
    "global_table.show(4, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_sdf_nine = last_minute_click_events.join(global_table, \n",
    "                                                    expr(\"\"\"\n",
    "                                                        (advertisement == choose_ad) AND\n",
    "                                                        (tsClick > tsAdvertisement ) AND\n",
    "                                                        (tsClick < tsAdvertisement + interval 60 seconds )\n",
    "                                                        \"\"\"\n",
    "                                                    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "q9 = (join_sdf_nine\n",
    "            .writeStream\n",
    "            .format(\"memory\")\n",
    "            .queryName(\"sinkTable\")\n",
    "            .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+-------------+--------------------------+\n",
      "|choose_ad|tsClick            |advertisement|tsAdvertisement           |\n",
      "+---------+-------------------+-------------+--------------------------+\n",
      "|G        |2022-04-26 10:51:45|G            |2022-04-26 10:51:27.604577|\n",
      "+---------+-------------------+-------------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM sinkTable ORDER BY tsClick DESC\").show(4,False) # note, I change ts in tsClick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "q9.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q10 - Left Stream-GlobalKTable Join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since **GlobalKTable** is not supportd in Apache SSS we can't do this join. But we can emulate it using a static dataframe like in the previous **Q9**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the static-df\n",
    "global_table = sc.parallelize([\n",
    "    ['A', time.time()],\n",
    "    ['B', time.time()],\n",
    "]\n",
    ").toDF([\"advertisement\",\"tsAdvertisement\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------------+\n",
      "|advertisement|tsAdvertisement           |\n",
      "+-------------+--------------------------+\n",
      "|A            |2022-04-26 10:53:02.702227|\n",
      "|B            |2022-04-26 10:53:02.702227|\n",
      "+-------------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "global_table = global_table.withColumn(\"tsAdvertisement\", to_timestamp(global_table[\"tsAdvertisement\"]))\n",
    "global_table.show(2, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_sdf_ten = last_minute_click_events.join(global_table, \n",
    "                                                    expr(\"\"\"\n",
    "                                                        (advertisement == choose_ad) AND\n",
    "                                                        (tsClick > tsAdvertisement ) AND\n",
    "                                                        (tsClick < tsAdvertisement + interval 20 seconds )\n",
    "                                                        \"\"\"\n",
    "                                                    ), \"leftOuter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "q10 = (join_sdf_ten\n",
    "            .writeStream\n",
    "            .format(\"memory\")\n",
    "            .queryName(\"sinkTable\")\n",
    "            .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+-------------+---------------+\n",
      "|choose_ad|tsClick            |advertisement|tsAdvertisement|\n",
      "+---------+-------------------+-------------+---------------+\n",
      "|B        |2022-04-26 10:52:33|null         |null           |\n",
      "|A        |2022-04-26 10:52:16|null         |null           |\n",
      "|G        |2022-04-26 10:51:45|null         |null           |\n",
      "|F        |2022-04-26 10:51:10|null         |null           |\n",
      "|D        |2022-04-26 10:50:53|null         |null           |\n",
      "|C        |2022-04-26 10:50:19|null         |null           |\n",
      "|B        |2022-04-26 10:49:42|null         |null           |\n",
      "|A        |2022-04-26 10:49:32|null         |null           |\n",
      "+---------+-------------------+-------------+---------------+\n",
      "only showing top 8 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM sinkTable ORDER BY tsClick DESC\").show(8,False) # note, I change ts in tsClick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "q10.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
